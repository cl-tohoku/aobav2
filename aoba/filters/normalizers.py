from os.path import dirname, join
import sys
import re
import unicodedata

import emoji
import mojimoji
import wikitextparser

sys.path.append(dirname(__file__))
from parsers import MecabParser


class SentenceNormalizer:
    def __init__(self):
        self.emoji = emoji.UNICODE_EMOJI["en"]
        self.parser = MecabParser()

    def __call__(self, text):
        text = self.normalize(text)
        text = self.parentheses(text)
        text = self.normalize_with_mecab(text)
        return text

    def normalize(self, text):
        text = unicodedata.normalize("NFKC", text)
        # special character
        text = re.sub("[\uA000-\uF8FF|\uFF3E|\u002A|\u005E|'\u0020']+", "", text)
        text = re.sub("[\u00C0-\u2E41]+", "", text)
        text = re.sub(r"(\.)+", "ã€‚", text)
        # emoji
        text = "".join([c for c in text if c not in self.emoji])
        return text

    @staticmethod
    def parentheses(text, patterns=["\(.+?\)", "\<.+?\>", "\[.+?]", "ï¼ˆ.+?ï¼‰"]):
        text = text.replace("<s>", "@@s@@")
        for pattern in patterns:
            text = re.sub(pattern, "", text)
        return text.replace("@@s@@", "<s>")

    def normalize_with_mecab(self, text):
        previous_word = ""
        output_text = []
        for meta in self.parser(text):
            word = meta.surface
            # 1å˜èªå†…ã§3æ–‡å­—ä»¥ä¸Šç¶šãå ´åˆã¯2æ–‡å­—ã«ã¾ã¨ã‚ã‚‹
            word = re.sub(r"(.){2, }", "\g<1>"*2, word)
            # 1å˜èªãŒ"ç¬‘","w"ã®ç¹°ã‚Šè¿”ã—ã®å ´åˆã¯å–ã‚Šé™¤ã
            word = re.sub(r"^(ç¬‘|w)*$", "", word)
            # åŒã˜å˜èªãŒ2å›ä»¥ä¸Šç¶šãå ´åˆã¯1å›ã«ã™ã‚‹
            next_word = word
            if next_word != previous_word:
                output_text.append(next_word)
                previous_word = next_word
        return "".join(output_text)

    def normalize_wikidump(self, text):
        # å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚«ãƒ†ã‚´ãƒªã®é™¤å»  https://qiita.com/FukuharaYohei/items/95a9ceaaf60858c3e483
        text = re.sub(r"""
            \[\[             # "[["(ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹)
            (?:              # ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾è±¡å¤–ã®ã‚°ãƒ«ãƒ¼ãƒ—é–‹å§‹
                [^|]*?       # "|"ä»¥å¤–ã®æ–‡å­—0æ–‡å­—ä»¥ä¸Šã€éè²ªæ¬²
                \|           # "|"
            )*?              # ã‚°ãƒ«ãƒ¼ãƒ—çµ‚äº†ã€ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒ0ä»¥ä¸Šå‡ºç¾ã€éè²ªæ¬²(No27ã¨ã®å¤‰æ›´ç‚¹)
            (                # ã‚°ãƒ«ãƒ¼ãƒ—é–‹å§‹ã€ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾è±¡
              (?!Category:)  # å¦å®šã®å…ˆèª­(å«ã‚“ã å ´åˆã¯å¯¾è±¡å¤–ã¨ã—ã¦ã„ã‚‹)
              ([^|]*?)    # "|"ä»¥å¤–ãŒ0æ–‡å­—ä»¥ä¸Šã€éè²ªæ¬²(è¡¨ç¤ºå¯¾è±¡ã®æ–‡å­—åˆ—)
            )
            \]\]        # "]]"ï¼ˆãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—çµ‚äº†ï¼‰        
            """, r"\1", text, flags=re.MULTILINE + re.VERBOSE)

        # é™¤å»å¯¾è±¡ï¼š{{lang|è¨€èªã‚¿ã‚°|æ–‡å­—åˆ—}}
        text = re.sub(r"""
            \{\{lang    # "{{lang"(ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹)
            (?:         # ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾è±¡å¤–ã®ã‚°ãƒ«ãƒ¼ãƒ—é–‹å§‹
                [^|]*?  # "|"ä»¥å¤–ã®æ–‡å­—ãŒ0æ–‡å­—ä»¥ä¸Šã€éè²ªæ¬²
                \|      # "|"
            )*?         # ã‚°ãƒ«ãƒ¼ãƒ—çµ‚äº†ã€ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒ0ä»¥ä¸Šå‡ºç¾ã€éè²ªæ¬²
            ([^|]*?)    # ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾è±¡ã€"|"ä»¥å¤–ãŒ0æ–‡å­—ä»¥ä¸Šã€éè²ªæ¬²(è¡¨ç¤ºå¯¾è±¡ã®æ–‡å­—åˆ—)
            \}\}        # "}}"(ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—çµ‚äº†)
            """, r"\1", text, flags=re.MULTILINE + re.VERBOSE)

        text = re.sub(r"('{5}|'{2,3})(.*?)('{5}|'{2,3})", r"\2", text)  # å¼·èª¿ã®å‰Šé™¤
        text = re.sub(r"(==+)(.*?)(==+)", "", text)  # è¦‹å‡ºã—
        text = re.sub(r"\((.*?)\)", "", text)  # "()"ã§æ›¸ã‹ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‹¬å¼§ã‚’å«ã‚ã¦å–ã‚Šé™¤ã
        text = re.sub(r"(\(|ï¼ˆ).*?(ï¼‰|\))", "", text)  # æ—¥æœ¬èªã®ãƒ•ã‚©ãƒ³ãƒˆã®å…¨è§’æ‹¬å¼§ï¼ˆï¼‰ã‚‚å–ã‚Šé™¤ã
        text = re.sub(r"<!--(.*?)-->", "", text)  # ã‚³ãƒ¡ãƒ³ãƒˆã®å‰Šé™¤
        text = re.sub(r"<.*>", "", text)  # htmlã‚¿ã‚°ç³»
        text = re.sub(r"\{\|class=.*?\|\}", "", text, flags=re.DOTALL)
        text = re.sub(r"(^(\*|#|;|:)+).*?$", "", text, flags=re.MULTILINE)  # ç®‡æ¡æ›¸ãã‚’å–ã‚Šé™¤ã
        text = re.sub(r"\[(http|https):\/\/(.*?)\]", "", text)  # å¤–éƒ¨ãƒªãƒ³ã‚¯ã®é™¤å»
        text = re.sub(r"(http|https):\/\/(.*)(.com|.org|.net|.int|.edu|.gov|.mil|.jp|.html)", "", text)  # å¤–éƒ¨ãƒªãƒ³ã‚¯ã®é™¤å»ãã®2(ä»–ã«ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒã‚ã£ãŸã‚‰è¿½è¨˜ã—ã¦ãã ã•ã„)
        text = re.sub(r"#REDIRECT \[\[(.*?)\]\]", "", text)  # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®å‰Šé™¤
        text = re.sub(r"~~~~", "", text)  # ç½²åã®å‰Šé™¤
        text = re.sub(r"\{\{.*\}\}", "", text)  # ã€Œç‰¹ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ã®å‰Šé™¤
        text = re.sub(r"^----", "", text)  # æ°´å¹³ç·šã®å‰Šé™¤

        text = wikitextparser.remove_markup(text)
        text = re.sub("^Category:.*?$", "", text, flags=re.MULTILINE)
        return text


if __name__ == "__main__":
    text = "ï¾”ï½¯ï¾ï½°...ï¼ˆç¬‘ï¼‰ç¬‘wwwğŸ˜„..."
    normalizer = SentenceNormalizer()
    normalized_text = normalizer(text)
    print(text)
    print(normalized_text)