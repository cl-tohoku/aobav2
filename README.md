# aobav2 bot

## Telegram ã§ã®å®Ÿè¡Œ

- [å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãƒ©ã‚¤ãƒ–ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼”#GettingStarted](https://dialog-system-live-competition.github.io/dslc4/gettingstart.html)

### æº–å‚™

```bash
# Mac ã®ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§
$ brew cask install telegram
```

### bot ä½œæˆæ‰‹é †

1. Telegram ã® bot æ¤œç´¢ç”»é¢ã§ã€ŒBotFatherã€ã‚’æ¤œç´¢
2. BotFather ã«å¯¾ã—ã¦ `/newbot` ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹
3. BotFather ã®è¿”ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¾“ã£ã¦ bot åãªã©ã‚’å…¥åŠ›
4. Done! Congratulations on your new bot... ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¥ãŸã‚‰ bot ä½œæˆå®Œäº†
5. API ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ§ãˆã¦ãŠãï¼ˆbot å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ï¼‰

### å®Ÿè¡Œ

TBA


## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```yaml
- aoba/:
    # å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤
    - filters/:
        - parsers.py:                       Mecab parser
        - normalizers.py:                   æ–‡æ­£è¦åŒ–ã‚„ Wikidump ã®ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
        - dialog_filter.py:                 ãƒ•ã‚£ãƒ«ã‚¿å¯¾è±¡ã‹åˆ¤å®šï¼ˆå˜èªæ•°/NGå˜èª/ã‹ã£ã“/ä»®åç‡ï¼‰
        - postprocess_scorer.py:            ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆJaccard/SIF/é‡è¤‡åº¦ï¼‰
        - bert_predictors/:
            - next_utterance_predictor.py:  NSP
            - nli_predictor.py:             JSNLI
    # giza+ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - giza/:
        - evaluate_trans.py:                ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡
        - backtrans.py:                     é¡ä¼¼åº¦é–¢é€£ã®è©•ä¾¡åŸºæº–ã‚’å®šç¾©
        - sbert.py:                         SentenceTransformer ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è©•ä¾¡ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
    # sentencepiece ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - spm/:
        - train_spm.py:
        - encode_spm.py:
        - detokenize_spm.py:
        - scripts/:
            - gather_twitter.sh:
            - apply_spm_to_dialog_datasets.sh:
    # dialogs
    - dialogs/:
        - ilys_fairseq/: TBA
        - nttcs_fairseq/: TBA
        - fid/: TBA
        - dialogpt/: TBA
        - wiki_template/:
            - datasets.yml:                 template_dialogue ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«
            - wiki_template_dialogue.py:    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”
    # knowledges
    - knowledges/:
        - dpr/: TBA
        - esearch/:
            - index_config.json:
            - index_config_sudachi.json:
            - register_docs_async_icu_normalizer.py:   jsonl ãƒ‡ãƒ¼ã‚¿ã‚’ ES ã«ç™»éŒ²ã™ã‚‹
            - es_search.py:                            ç™»éŒ²ã—ãŸ index_name ã‚’ç”¨ã„ã¦æ¤œç´¢
    # telegram
    - telegrams/: TBA

# å‰å‡¦ç†ãªã©ã§å‚ç…§ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ç½®ã‹ãªã„ï¼‰
- data/:
    - ng_words.txt:                         NG å˜èªãƒªã‚¹ãƒˆ
    - wiki_template_dialog/:
        - context_templates.json:           ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¯¾è©±ã«å¿…è¦
        - response_templates.jsonl:         ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¯¾è©±ã«å¿…è¦

# å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- scripts/:
    - set_*.sh:                             ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–¢é€£
    - download_*.sh:                        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£
    - prepro_*.sh:                          å‰å‡¦ç†é–¢é€£
    - register_wikidump.sh:                 wikidump ã‚’ ES ã«ç™»éŒ²ã™ã‚‹
    - run_giza.sh:                          GIZA++ ã‚’ç”¨ã„ã¦ A3 ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

# å‰å‡¦ç†
- datasets.yml:                             ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨èª­ã¿è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- prepro/:
    - convert_format.py:                    List[Dialog] ã®å½¢å¼ã«å¤‰æ›ã™ã‚‹ï¼ˆDialog = ["ã“ã‚“ã«ã¡ã¯", "ã„ã„å¤©æ°—ã§ã™ã­", ...]ï¼‰
    - formats/:
        - base.py:
        - dailydialog.py:                   DailyDialog ãƒ‡ãƒ¼ã‚¿ List[Dialog] ã®å½¢å¼ã§èª­ã¿è¾¼ã‚€

# lib
- lib/:
    - elasticsearch-7.10.0/:                scripts/set_elasticsearch.sh
    - giza-pp/:                             scripts/set_giza.sh
```

## filters

```py
from aoba import (
    MecabParser,
    SentenceNormalizer,
    NextUtterancePredictor, 
    JsnliPredictor,
)

# MecabParser
parser = MecabParser()
parsed_text = parser("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚")

# SentenceNormalizer
normalizer = SentenceNormalizer()
normalized_text = normalizer("ï¾”ï½¯ï¾ï½°...ï¼ˆç¬‘ï¼‰ç¬‘wwwğŸ˜„...")

# NextUtterancePredictor
predictor = NextUtterancePredictor("/work01/slud_livechat_2020/mlm-checkpoint-43000-pytorch-model.bin")
results = predictor(
    ["ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­", "å¤–ã«éŠã³ã«ã„ãã¾ã—ã‚‡ã†"],     # contexts
    ["ã„ã„ã§ã™ã­", "ã©ã“ã„ãã¾ã™ã‹ï¼Ÿ"]                   # response_candidates
)

# JsnliPredictor
predictor = JsnliPredictor("/work02/SLUD2021/github/src/submodules/jsnli/outputs/best-24000")
result = predictor([
    [
        "ãƒ¯ã‚¯ãƒãƒ³æ‰“ã£ãŸã‚‰å‰¯ä½œç”¨ãŒè¾›ã‹ã£ãŸã€‚",  # premise
        "å…·ä½“çš„ã«ã©ã‚“ãªå‰¯ä½œç”¨ãŒã‚ã‚Šã¾ã—ãŸï¼Ÿ"   # hypothesis
    ]
])
```

## giza

```bash
$ bash scripts/set_giza.sh
$ bash scripts/run_giza.sh {fi_src} {fi_tgt} {dest}
```

```bash
# {a3ãƒ•ã‚¡ã‚¤ãƒ«, backtransãƒ•ã‚¡ã‚¤ãƒ«} ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®æ±ºå®š
$ python aoba/giza/evaluate_trans.py \
    --a3_file {a3_file} \
    --backtrans_file {backtrans_file} \
    --output_file {output_file}
```

```py
from aoba import (
    TransEvaluator
)

# backtrans è©•ä¾¡
evaluator = TransEvaluator()
result = evaluator(
    "she was interested in world history because she read the book", # source
    "she read the book because she was interested in world history"  # target
)
```

## dialogs

```py
from aoba import (
    WikipediaTemplateDialogue
)

# Wikipedia ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”
template_dialogue = WikipediaTemplateDialogue()
response = template_dialogue("æ±äº¬ã‚¿ãƒ¯ãƒ¼ã£ã¦çŸ¥ã£ã¦ã‚‹ï¼Ÿ")
```


# å‰å‡¦ç†

## Twitter ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- https://io-lab.esa.io/posts/2193

```bash
# æ”¹è‰¯ç‰ˆï¼ˆå®Ÿè¡Œå†…å®¹ã¯å¤‰ã‚ã£ã¦ã„ãªã„ã€‚å®Ÿè¡Œå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ï¼‰
bash scripts/prepro_twitter.sh {fi_context} {fi_response} {year}
```

## 2020 (NII/LINE)

- Basicãƒ•ã‚£ãƒ«ã‚¿
  - [x] __URL__
    - `re.compile(r"https?://[\w/:%#\$&\?\(\)~\.=\+\-]+")`
  - [x] __ãƒ¦ãƒ¼ã‚¶å__
    - `re.compile(r"@[a-zA-Z0-9_]{1,15}")`
  - [x] __ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°__
  - [x] __æ—¥æœ¬èªã‚’å«ã¾ãªã„æ–‡__
    - `unicodedata.name`ãŒã€ŒCJK UNIFIED, HIRAGANA, KATAKANAã€ã®ã„ãšã‚Œã‹ã§ã‚ã‚Œã°æ—¥æœ¬èªã®æ–‡å­—ã¨ã™ã‚‹
- é¡”æ–‡å­—ãƒ•ã‚£ãƒ«ã‚¿
  - [x] __é¡”æ–‡å­—æ¤œå‡º__
    1. æ­£è¦è¡¨ç¾ã® "ï¼»\W\_a-zA-Z]+" ã«è©²å½“ã™ã‚‹æ–‡å­—åˆ—ã‚’é¡”æ–‡å­—å€™è£œ X ã¨ã™ã‚‹ã€‚ä»¥ä¸‹ã€Xã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†ã€‚
    2. ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚’strip ("twitter?"ï¼Œ"goodï¼ï¼ï¼Œ"ã€Œthis"ãªã©ã¯é¡”æ–‡å­—ã§ã¯ãªã„)
    3. "ã€‚ï¼.ã€ï¼Œ,ãƒ»ï½¥â€¦ã€œ~-ï¼ï¼Ÿ!?" ã®ç¹°ã‚Šè¿”ã—ã‚’1æ–‡å­—ã¨ã¿ãªã™
    4. Xã«"ï¼»\W\_]" ãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‹ã‚‰ãªã‚‹æ–‡å­—åˆ—ãªã®ã§é¡”æ–‡å­—å€™è£œã‹ã‚‰é™¤å¤–
    5. 1~4ã‚’è¡Œã£ãŸã®ã¡ï¼ŒXã®é•·ã•ãŒ3ä»¥ä¸Šã®ã‚‚ã®ã¯é¡”æ–‡å­—ã¨ã¿ãªã™
    6. é¡”æ–‡å­—å€™è£œ Xã§ãªãã¨ã‚‚ï¼Œ"()"ã¾ãŸã¯"ï¼ˆï¼‰"ã§å›²ã¾ã‚Œã¦ãŠã‚Šï¼Œå†…éƒ¨ã«æ—¥æœ¬èªãŒå­˜åœ¨ã—ãªã„ã‚‚ã®ã¯é™¤å¤–
    7. 6ã¯ãŸã¨ãˆæ—¥æœ¬èªã§ã‚ã£ã¦ã‚‚"T, o, O, ãƒ­, å£, ï¾›, ã¤, ã£, ç¬, ãƒ, ï¾‰, c, C"ä»¥å¤–ã®æ–‡å­—ã¨ä¸€å›ã¯ãƒãƒƒãƒã™ã‚‹å¿…è¦ã‚ã‚Š
- ãã®ä»–
  - [x] __ãƒˆãƒ¼ã‚¯ãƒ³æ•°__
    - ï¼‘ç™ºè©±ã®å˜èªæ•°ãŒ6~29ã«åã¾ã‚‰ãªã„ã‚‚ã®ã¯é™¤å¤–
    - mecabã€è¾æ›¸ã¯`/opt/local/lib/mecab/dic/naist-jdic/sys.dic`
    - BPE tokenæ•°ãŒ128ã‚’è¶…ãˆã‚‹ã¾ã§contextã‚’ä½¿ç”¨
  - [x] __é‡è¤‡åº¦__
    - Jaccard similarity ãŒ é–¾å€¤ 0.5 ã‚’è¶…ãˆãŸã‚‰é™¤å¤–
  - [x] __ç¹°ã‚Šè¿”ã—è¡¨ç¾__
    - len(set(words)) / len(words) ãŒ 0.5ã‚’ä¸‹å›ã£ãŸã‚‰é™¤å¤–