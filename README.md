# aobav2 bot

```bash
git clone --recursive git@github.com:cl-tohoku/aobav2.git
# git submodule update --init --recursive
```

## Telegram ã§ã®å®Ÿè¡Œ

[./deploy](./deploy) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‚ç…§


## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```yaml
- aoba/:
    # å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤
    - filters/:
        - parsers.py:                       Mecab parser
        - normalizers.py:                   æ–‡æ­£è¦åŒ–ã‚„ Wikidump ã®ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
        - dialog_filter.py:                 ãƒ•ã‚£ãƒ«ã‚¿å¯¾è±¡ã‹åˆ¤å®šï¼ˆå˜èªæ•°/NGå˜èª/ã‹ã£ã“/ä»®åç‡ï¼‰
        - postprocess_scorer.py:            ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆJaccard/SIF/é‡è¤‡åº¦ï¼‰
        - bert_predictors/:
            - next_utterance_predictor.py:  NSP
            - nli_predictor.py:             JSNLI
    # giza+ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - giza/:
        - evaluate_trans.py:                ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡
        - backtrans.py:                     é¡ä¼¼åº¦é–¢é€£ã®è©•ä¾¡åŸºæº–ã‚’å®šç¾©
        - sbert.py:                         SentenceTransformer ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è©•ä¾¡ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
    # sentencepiece ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - spm/:
        - train_spm.py:
        - encode_spm.py:
        - detokenize_spm.py:
        - scripts/:
            - gather_twitter.sh:
            - apply_spm_to_dialog_datasets.sh:
    # dialogs
    - dialogs/:
        - ilys_fairseq/: TBA
        - nttcs_fairseq/: TBA
        - fusion_in_decoder/: TBA
        - dialogpt/: TBA
        - wiki_template/:
            - datasets.yml:                 template_dialogue ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«
            - wiki_template_dialogue.py:    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”
    # knowledges
    - knowledges/:
        - dpr/: TBA
        - esearch/:
            - index_config.json:
            - index_config_sudachi.json:
            - register_docs_async_icu_normalizer.py:   jsonl ãƒ‡ãƒ¼ã‚¿ã‚’ ES ã«ç™»éŒ²ã™ã‚‹
            - es_search.py:                            ç™»éŒ²ã—ãŸ index_name ã‚’ç”¨ã„ã¦æ¤œç´¢

# å‰å‡¦ç†ãªã©ã§å‚ç…§ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ç½®ã‹ãªã„ï¼‰
- data/:
    - ng_words.txt:                         NG å˜èªãƒªã‚¹ãƒˆ
    - wiki_template_dialog/:
        - context_templates.json:           ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¯¾è©±ã«å¿…è¦
        - response_templates.jsonl:         ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¯¾è©±ã«å¿…è¦

# å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- scripts/:
    - set_*.sh:                             ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–¢é€£
    - download_*.sh:                        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£
    - prepro_*.sh:                          å‰å‡¦ç†é–¢é€£
    - register_wikidump.sh:                 wikidump ã‚’ ES ã«ç™»éŒ²ã™ã‚‹
    - run_giza.sh:                          GIZA++ ã‚’ç”¨ã„ã¦ A3 ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

# å‰å‡¦ç†
- datasets.yml:                             ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨èª­ã¿è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- prepro/:
    - create_parallel_corpus.py:            .context/.response ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

# telegram
- deploy/:
    - run_telegram.py:
    - telegrams/:

# lib
- lib/:
    - elasticsearch-7.10.0/:                scripts/set_elasticsearch.sh
    - giza-pp/:                             scripts/set_giza.sh
```

## å‰å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

```py
from aoba import (
    MecabParser,
    SentenceNormalizer,
    NextUtterancePredictor, 
    JsnliPredictor,
)

# MecabParser
parser = MecabParser()
parsed_text = parser("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚")

# SentenceNormalizer
normalizer = SentenceNormalizer()
normalized_text = normalizer("ï¾”ï½¯ï¾ï½°...ï¼ˆç¬‘ï¼‰ç¬‘wwwğŸ˜„...")

# NextUtterancePredictor
predictor = NextUtterancePredictor("/work01/slud_livechat_2020/mlm-checkpoint-43000-pytorch-model.bin")
results = predictor(
    ["ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­", "å¤–ã«éŠã³ã«ã„ãã¾ã—ã‚‡ã†"],     # contexts
    ["ã„ã„ã§ã™ã­", "ã©ã“ã„ãã¾ã™ã‹ï¼Ÿ"]                   # response_candidates
)

# JsnliPredictor
predictor = JsnliPredictor("/work02/SLUD2021/github/src/submodules/jsnli/outputs/best-24000")
result = predictor([
    [
        "ãƒ¯ã‚¯ãƒãƒ³æ‰“ã£ãŸã‚‰å‰¯ä½œç”¨ãŒè¾›ã‹ã£ãŸã€‚",  # premise
        "å…·ä½“çš„ã«ã©ã‚“ãªå‰¯ä½œç”¨ãŒã‚ã‚Šã¾ã—ãŸï¼Ÿ"   # hypothesis
    ]
])
```

## giza

```bash
$ bash scripts/set_giza.sh
$ bash scripts/run_giza.sh {fi_src} {fi_tgt} {dest}
```

```bash
# {a3ãƒ•ã‚¡ã‚¤ãƒ«, backtransãƒ•ã‚¡ã‚¤ãƒ«} ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®æ±ºå®š
$ python aoba/giza/evaluate_trans.py \
    --a3_file {a3_file} \
    --backtrans_file {backtrans_file} \
    --output_file {output_file}
```

```py
from aoba import (
    TransEvaluator
)

# backtrans è©•ä¾¡
evaluator = TransEvaluator()
result = evaluator(
    "she was interested in world history because she read the book", # source
    "she read the book because she was interested in world history"  # target
)
```

## çŸ¥è­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

### Elastic Search

* [./aoba/knowledges/esearch/README.md](./aoba/knowledges/esearch/README.md) ã‚’å‚ç…§
* ElasticStack ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®æ›¸ç±ãŒãŠã™ã™ã‚ï¼ˆpython ã«ã‚ˆã‚‹è§£èª¬ã¯æ®‹å¿µãªãŒã‚‰ãªã„ï¼‰
  * [ElasticStack å®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼ˆAmazonï¼‰](https://www.amazon.co.jp/-/en/%E6%83%A3%E9%81%93-%E5%93%B2%E4%B9%9F/dp/4295009776/ref=pd_lpo_1?pd_rd_i=4295009776&psc=1)


### Dense Passage Retrieval

* https://github.com/cl-tohoku/AIO2_DPR_baseline

```python
from omegaconf import OmegaConf
from aoba import DenseExtractor

cfg = OmegaConf.load(open("aoba/knowledges/dense_passage_retrieval/interact_retriever.yml"))
dense_extractor = DenseExtractor(cfg)

query = "æ±äº¬éƒ½æ¸¯åŒºèŠå…¬åœ’ã«ã‚ã‚‹ç·åˆé›»æ³¢å¡”ã®åå‰ã¯ä½•ï¼Ÿ"
retrieved_passages = dense_extractor(query, n_docs=5)
print(retrieved_passages[0])

{
    'id': 'wiki:1212368',
    'score': 44.463657,
    'title': 'æ±äº¬ã‚¿ãƒ¯ãƒ¼',
    'text': 'æ±äº¬ã‚¿ãƒ¯ãƒ¼ã¯ã€æ—¥æœ¬ã®æ±äº¬éƒ½æ¸¯åŒºèŠå…¬åœ’ã«ã‚ã‚‹ç·åˆé›»æ³¢å¡”ã®æ„›ç§°ã§ã‚ã‚‹ã€‚æ­£å¼åç§°ã¯æ—¥æœ¬é›»æ³¢å¡”ã€‚å‰µè¨­è€…ã¯å‰ç”°ä¹…å‰ã€‚'
}
```

## å¯¾è©±ãƒ¢ãƒ‡ãƒ«

### Wikipedia ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”

```py
from aoba import WikipediaTemplateDialogue

template_dialogue = WikipediaTemplateDialogue()
response = template_dialogue("æ±äº¬ã‚¿ãƒ¯ãƒ¼ã£ã¦çŸ¥ã£ã¦ã‚‹ï¼Ÿ")
```

### Fusion-in-Decoder

```py
import argparse
from omegaconf import OmegaConf
from aoba import DenseExtractor, FidModel

cfg = OmegaConf.load(open("aoba/knowledges/dense_passage_retrieval/interact_retriever.yml"))
dense_extractor = DenseExtractor(cfg)

parser = argparse.ArgumentParser()
parser = FidModel.add_parser(parser)
args = parser.parse_args()
decoder = FidModel(args)

query = "æ±äº¬éƒ½æ¸¯åŒºã«ã‚ã‚‹æ±äº¬ã‚¿ãƒ¯ãƒ¼ã¯ä½•ã®å»ºç‰©ã§ã™ã‹ï¼Ÿ"
retrieved_passages = dense_extractor(query, n_docs=5)
input_data = FidModel.convert_retrieved_psgs(query, retrieved_passages)
responses = decoder(input_data)
responses[0]

"æ±äº¬éƒ½æ¸¯åŒºèŠå…¬åœ’ã«ã‚ã‚‹ç·åˆé›»æ³¢å¡”ã®æ„›ç§°ã§ã‚ã‚‹ã€‚æ­£å¼åç§°ã¯æ—¥æœ¬é›»æ³¢å¡”ã€‚å‰µè¨­è€…ã¯å‰ç”°ä¹…å‰ã€‚"
```

### DialoGPT

```py
import argparse
from aoba import DialoGptModel

parser = argparse.ArgumentParser(description="")
parser = DialoGptModel.add_parser(parser)
args = parser.parse_args()

decoder = DialoGptModel(args)

history = ["ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™"]
response = decoder(history, num_beams=5)
responses[0]

"ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚"
```


# å‰å‡¦ç†

## Twitter ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- https://io-lab.esa.io/posts/2193

```bash
# æ”¹è‰¯ç‰ˆï¼ˆå®Ÿè¡Œå†…å®¹ã¯å¤‰ã‚ã£ã¦ã„ãªã„ã€‚å®Ÿè¡Œå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ï¼‰
bash scripts/prepro_twitter.sh {fi_context} {fi_response} {year}
```

## 2020 (NII/LINE)

- Basicãƒ•ã‚£ãƒ«ã‚¿
  - [x] __URL__
    - `re.compile(r"https?://[\w/:%#\$&\?\(\)~\.=\+\-]+")`
  - [x] __ãƒ¦ãƒ¼ã‚¶å__
    - `re.compile(r"@[a-zA-Z0-9_]{1,15}")`
  - [x] __ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°__
  - [x] __æ—¥æœ¬èªã‚’å«ã¾ãªã„æ–‡__
    - `unicodedata.name`ãŒã€ŒCJK UNIFIED, HIRAGANA, KATAKANAã€ã®ã„ãšã‚Œã‹ã§ã‚ã‚Œã°æ—¥æœ¬èªã®æ–‡å­—ã¨ã™ã‚‹
- é¡”æ–‡å­—ãƒ•ã‚£ãƒ«ã‚¿
  - [x] __é¡”æ–‡å­—æ¤œå‡º__
    1. æ­£è¦è¡¨ç¾ã® "ï¼»\W\_a-zA-Z]+" ã«è©²å½“ã™ã‚‹æ–‡å­—åˆ—ã‚’é¡”æ–‡å­—å€™è£œ X ã¨ã™ã‚‹ã€‚ä»¥ä¸‹ã€Xã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†ã€‚
    2. ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚’strip ("twitter?"ï¼Œ"goodï¼ï¼ï¼Œ"ã€Œthis"ãªã©ã¯é¡”æ–‡å­—ã§ã¯ãªã„)
    3. "ã€‚ï¼.ã€ï¼Œ,ãƒ»ï½¥â€¦ã€œ~-ï¼ï¼Ÿ!?" ã®ç¹°ã‚Šè¿”ã—ã‚’1æ–‡å­—ã¨ã¿ãªã™
    4. Xã«"ï¼»\W\_]" ãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‹ã‚‰ãªã‚‹æ–‡å­—åˆ—ãªã®ã§é¡”æ–‡å­—å€™è£œã‹ã‚‰é™¤å¤–
    5. 1~4ã‚’è¡Œã£ãŸã®ã¡ï¼ŒXã®é•·ã•ãŒ3ä»¥ä¸Šã®ã‚‚ã®ã¯é¡”æ–‡å­—ã¨ã¿ãªã™
    6. é¡”æ–‡å­—å€™è£œ Xã§ãªãã¨ã‚‚ï¼Œ"()"ã¾ãŸã¯"ï¼ˆï¼‰"ã§å›²ã¾ã‚Œã¦ãŠã‚Šï¼Œå†…éƒ¨ã«æ—¥æœ¬èªãŒå­˜åœ¨ã—ãªã„ã‚‚ã®ã¯é™¤å¤–
    7. 6ã¯ãŸã¨ãˆæ—¥æœ¬èªã§ã‚ã£ã¦ã‚‚"T, o, O, ãƒ­, å£, ï¾›, ã¤, ã£, ç¬, ãƒ, ï¾‰, c, C"ä»¥å¤–ã®æ–‡å­—ã¨ä¸€å›ã¯ãƒãƒƒãƒã™ã‚‹å¿…è¦ã‚ã‚Š
- ãã®ä»–
  - [x] __ãƒˆãƒ¼ã‚¯ãƒ³æ•°__
    - ï¼‘ç™ºè©±ã®å˜èªæ•°ãŒ6~29ã«åã¾ã‚‰ãªã„ã‚‚ã®ã¯é™¤å¤–
    - mecabã€è¾æ›¸ã¯`/opt/local/lib/mecab/dic/naist-jdic/sys.dic`
    - BPE tokenæ•°ãŒ128ã‚’è¶…ãˆã‚‹ã¾ã§contextã‚’ä½¿ç”¨
  - [x] __é‡è¤‡åº¦__
    - Jaccard similarity ãŒ é–¾å€¤ 0.5 ã‚’è¶…ãˆãŸã‚‰é™¤å¤–
  - [x] __ç¹°ã‚Šè¿”ã—è¡¨ç¾__
    - len(set(words)) / len(words) ãŒ 0.5ã‚’ä¸‹å›ã£ãŸã‚‰é™¤å¤–
