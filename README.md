# aobav2 bot


## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```yaml
- aoba/:
    # å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤
    - filters/:
        - parsers.py:                       Mecab parser
        - normalizers.py:                   æ–‡æ­£è¦åŒ–ã‚„ Wikidump ã®ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
        - bert_predictors/:
            - next_utterance_predictor.py:  NSP
            - nli_predictor.py:             JSNLI
    # giza+ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - giza/:
        - evaluate_trans.py:                ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆè©•ä¾¡
        - backtrans.py:                     ã„ã‚ã„ã‚ãªè©•ä¾¡åŸºæº–ã‚’å®šç¾©
        - sbert.py:                         SentenceTransformer ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è©•ä¾¡ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰
    # sentencepiece ã«é–¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤
    - spm/:
        - train_spm.py:
        - encode_spm.py:
        - detokenize_spm.py:
        - scripts/:
            - gather_twitter.sh:
            - apply_spm_to_dialog_datasets.sh:
    # dialogs
    - dialogs/:
        - wiki_template/:
            - datasets.yml:                 template_dialogue ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«
            - wiki_template_dialogue.py:    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”

# å‰å‡¦ç†ãªã©ã§å‚ç…§ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ç½®ã‹ãªã„ï¼‰
- data/:
    - ng_words.txt:                         NG å˜èªãƒªã‚¹ãƒˆ

# å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- scripts/:
    - set_*.sh:                             ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–¢é€£
    - download_*.sh:                        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢é€£
    - prepro_*.sh:                          å‰å‡¦ç†é–¢é€£

# å‰å‡¦ç†
- datasets.yml:                             ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨èª­ã¿è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- prepro/:
    - convert_format.py:                    List[Dialog] ã®å½¢å¼ã«å¤‰æ›ã™ã‚‹ï¼ˆDialog = ["ã“ã‚“ã«ã¡ã¯", "ã„ã„å¤©æ°—ã§ã™ã­", ...]ï¼‰
    - formats/:
        - base.py:
        - dailydialog.py:                   DailyDialog ãƒ‡ãƒ¼ã‚¿ List[Dialog] ã®å½¢å¼ã§èª­ã¿è¾¼ã‚€
```

## filters

```py
from aoba import (
    MecabParser,
    SentenceNormalizer,
    NextUtterancePredictor, 
    JsnliPredictor,
)

# MecabParser
parser = MecabParser()
parsed_text = parser("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚")

# SentenceNormalizer
normalizer = SentenceNormalizer()
normalized_text = normalizer("ï¾”ï½¯ï¾ï½°...ï¼ˆç¬‘ï¼‰ç¬‘wwwğŸ˜„...")

# NextUtterancePredictor
predictor = NextUtterancePredictor("/work01/slud_livechat_2020/mlm-checkpoint-43000-pytorch-model.bin")
results = predictor(
    ["ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­", "å¤–ã«éŠã³ã«ã„ãã¾ã—ã‚‡ã†"],     # contexts
    ["ã„ã„ã§ã™ã­", "ã©ã“ã„ãã¾ã™ã‹ï¼Ÿ"]                   # response_candidates
)

# JsnliPredictor
predictor = JsnliPredictor("/work02/SLUD2021/github/src/submodules/jsnli/outputs/best-24000")
result = predictor([
    [
        "ãƒ¯ã‚¯ãƒãƒ³æ‰“ã£ãŸã‚‰å‰¯ä½œç”¨ãŒè¾›ã‹ã£ãŸã€‚",  # premise
        "å…·ä½“çš„ã«ã©ã‚“ãªå‰¯ä½œç”¨ãŒã‚ã‚Šã¾ã—ãŸï¼Ÿ"   # hypothesis
    ]
])
```

## giza

```bash
$ bash scripts/set_giza.sh
$ bash scripts/run_giza.sh {fi_src} {fi_tgt} {dest}
```

```py
from aoba import (
    TransEvaluator
)

# backtrans è©•ä¾¡
evaluator = TransEvaluator()
result = evaluator(
    "she was interested in world history because she read the book", # source
    "she read the book because she was interested in world history"  # target
)
```

## dialogs

```py
from aoba import (
    WikipediaTemplateDialogue
)

# Wikipedia ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”
template_dialogue = WikipediaTemplateDialogue()
response = template_dialogue("æ±äº¬ã‚¿ãƒ¯ãƒ¼ã£ã¦çŸ¥ã£ã¦ã‚‹ï¼Ÿ")
```